<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yurl2018.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="感知器一个感知器需要几个二进制输入， $x_1$, $x_2$, $x_3$, …，并产生一个二进制输出：">
<meta property="og:type" content="article">
<meta property="og:title" content="Using neural nets to recognize handwritten digits">
<meta property="og:url" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/index.html">
<meta property="og:site_name" content="footprint of study">
<meta property="og:description" content="感知器一个感知器需要几个二进制输入， $x_1$, $x_2$, $x_3$, …，并产生一个二进制输出：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/digits.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/1.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/2.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/3.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/4.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/5.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/6.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/digits_separate.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_first_digit.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/6.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/7.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/8.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_complete_zero.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/9.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/valley.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/valley_with_ball.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_2_and_1.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_really_bad_images.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/10.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/11.png">
<meta property="og:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/12.png">
<meta property="article:published_time" content="2023-03-30T05:19:19.000Z">
<meta property="article:modified_time" content="2023-06-12T07:00:32.272Z">
<meta property="article:author" content="yurl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/digits.png">


<link rel="canonical" href="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/","path":"2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/","title":"Using neural nets to recognize handwritten digits"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Using neural nets to recognize handwritten digits | footprint of study</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">footprint of study</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">WTF</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-number">1.</span> <span class="nav-text">感知器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid-%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="nav-number">2.</span> <span class="nav-text">Sigmoid 神经元</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">神经网络架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E5%AF%B9%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E7%9A%84%E7%AE%80%E5%8D%95%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">一个对手写数字进行分类的简单网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.</span> <span class="nav-text">梯度下降学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E6%88%91%E4%BB%AC%E7%9A%84%E7%BD%91%E7%BB%9C%E5%AF%B9%E6%95%B0%E5%AD%97%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB"><span class="nav-number">6.</span> <span class="nav-text">实现我们的网络对数字进行分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%B0%E5%90%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">7.</span> <span class="nav-text">走向深度学习</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">yurl</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yurl2018.github.io/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yurl">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="footprint of study">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Using neural nets to recognize handwritten digits | footprint of study">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Using neural nets to recognize handwritten digits
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-03-30 13:19:19" itemprop="dateCreated datePublished" datetime="2023-03-30T13:19:19+08:00">2023-03-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-12 15:00:32" itemprop="dateModified" datetime="2023-06-12T15:00:32+08:00">2023-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h2><p>一个感知器需要几个二进制输入， $x_1$, $x_2$, $x_3$, …，并产生一个二进制输出：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/digits.png" class="" title="感知器">

<span id="more"></span>

<p>例子中输入为<code>3</code>个（可以有更多或更少的输入，由加权和$\sum_j w_j x_j$大于或小于<code>threshold</code>来决定输出要么<code>0</code>要么<code>1</code>）<code>threshold</code>是一个实数，他是神经元的一个参数。<br>设权重为 $w_1$, $w_2$, $w_3$</p>
<p>\begin{eqnarray}<br>  \mbox{output} &amp; &#x3D; &amp; \left{ \begin{array}{ll}<br>      0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \<br>      1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold}<br>      \end{array} \right.<br>\tag{1}\end{eqnarray}</p>
<p>以上这就是感知器如何工作的全部内容，这就是基本的数学模型。您可以考虑感知器的一种方式是，它是一种通过权衡证据做出决策的设备。<br>简化描述感知器的方式： 条件$\sum_j w_j x_j &gt; \mbox{threshold}$很麻烦，用两个符号来简化他， 第一个变化是$\sum_j w_j x_j$作为一个点积，$w \cdot x \equiv \sum_j w_j x_j$，其中<code>w</code> <code>x</code>是向量，其分量分别是权重和输入，第二个变化是将阈值移到不等式的另一边，并用所谓的感知器偏差代替它 $b \equiv<br>-\mbox{threshold}$使用偏差而不是阈值，感知器规则可以重写:</p>
<p>\begin{eqnarray}<br>  \mbox{output} &#x3D; \left{<br>    \begin{array}{ll}<br>      0 &amp; \mbox{if } w\cdot x + b \leq 0 \<br>      1 &amp; \mbox{if } w\cdot x + b &gt; 0<br>    \end{array}<br>  \right.<br>\tag{2}\end{eqnarray}</p>
<p>您可以将偏差视为衡量感知器输出<code>1</code>的难易程度。或者用更生物学的术语来说，偏差是衡量感知器 启动难易程度的指标。</p>
<h2 id="Sigmoid-神经元"><a href="#Sigmoid-神经元" class="headerlink" title="Sigmoid 神经元"></a>Sigmoid 神经元</h2><p>事实上，网络中任何单个感知器的权重或偏差的微小变化有时会导致该感知器的输出完全翻转，例如从0到1. 这种翻转可能会导致网络其余部分的行为以某种非常复杂的方式完全改变。因此，虽然您的“9”现在可能被正确分类，但所有其他图像上的网络行为可能已经以某种难以控制的方式完全改变。这使得很难看到如何逐渐修改权重和偏差，以使网络更接近所需的行为。也许有一些巧妙的方法可以解决这个问题。但我们如何让感知器网络进行学习并不是很明显。</p>
<p>我们可以通过引入一种称为sigmoid神经元的新型人工神经元来克服这个问题。Sigmoid神经元类似于感知器，但经过修改后，它们的权重和偏差的微小变化只会导致其输出的微小变化。这是允许 sigmoid 神经元网络学习的关键事实。</p>
<p>好的，让我描述一下 sigmoid 神经元。我们将以与描述感知器相同的方式描述 sigmoid 神经元：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/1.png" class="" title="神经元">

<p>就像感知器一样，sigmoid 神经元有输入，$x_1$, $x_2$, $x_3$, …，但不仅仅是<code>0</code>或<code>1</code>，这些输入也可以取<code>0</code>和<code>1</code>之间的任何值. 所以，例如， <code>0.638…</code>是 sigmoid 神经元的有效输入。就像感知器一样，sigmoid 神经元对每个输入都有权重，$w_1$, $w_2$, $w_3$ , 以及总体偏差<code>b</code>. 但是输出不是<code>0</code>或<code>1</code>，而是$\sigma(w \cdot x+b)$, <code>σ</code>称为 sigmoid 函数，并由下式定义：</p>
<p>\begin{eqnarray}<br>  \sigma(z) \equiv \frac{1}{1+e^{-z}}<br>\tag{3}\end{eqnarray}</p>
<p>更明确地说，带有输入的 sigmoid 神经元的输出$x_1$, $x_2$, $x_3$, …，权重$w_1$, $w_2$, $w_3$ , 和偏差<code>b</code>是:</p>
<p>\begin{eqnarray}<br>  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}<br>\tag{4}\end{eqnarray}</p>
<p>乍一看，sigmoid 神经元似乎与感知器非常不同。如果您还不熟悉，sigmoid 函数的代数形式可能看起来不透明且令人生畏。事实上，感知器和 sigmoid 神经<br>元之间有许多相似之处，sigmoid 函数的代数形式更多地是技术细节，而不是真正的理解障碍。</p>
<p>为了理解与感知器模型的相似性，假设$z\equiv w \cdot x + b$是一个很大的正数。然后$e^{-z}\approx 0$ 所以$\sigma(z) \approx 1$. 换句话说，当$z &#x3D; w<br>\cdot x+b$为大且为正，Sigmoid 神经元的输出约为<code>1</code>，就像感知器一样。假设另一方面$z &#x3D; w \cdot x+b$是非常小的负值。那么$e^{-z} \rightarrow \infty$，并且$\sigma(z) \approx 0$. 所以当$z &#x3D; w \cdot x+b$是负值时，Sigmoid 神经元的行为也非常接近感知器。只有当$w \cdot x+b$大小适中，与感知器模型有很大偏差。</p>
<p><code>σ</code>的代数形式是什么? 我们怎么能理解呢？事实上，<code>σ</code>确切的形式不是那么重要 - 真正重要的是绘制时函数的形状。这是形状：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/2.png" class="" title="sigmoid 函数">

<p>此形状是阶跃函数的平滑版本：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/3.png" class="" title="step 函数">

<p>如果<code>σ</code>实际上是一个阶跃函数，那么 sigmoid 神经元将是一个感知器，因为输出将是<code>1</code>或<code>0</code>取决于是否$w\cdot x+b$是正负上，通过使用实际<code>σ</code>函数正如上面已经暗示的那样，我们得到一个平滑的感知器。确实，这就是光滑度<code>σ</code>函数是关键的事实，而不是它的详细形式。的平滑度<code>σ</code>意味着微小的变化$Δw_j$在权重和$Δb$在偏差会产生一个小的变化$Δoutput$在神经元的输出中。事实上，微积分告诉我们$Δoutput$输出很好地近似于</p>
<p>\begin{eqnarray}<br>  \Delta \mbox{output} \approx \sum_j \frac{\partial , \mbox{output}}{\partial w_j}<br>  \Delta w_j + \frac{\partial , \mbox{output}}{\partial b} \Delta b,<br>\tag{5}\end{eqnarray}</p>
<p>其中总和超过所有权重$w_j$， 和$\partial \mbox{output} &#x2F; \partial w_j$ 和$\partial \mbox{output} &#x2F;\partial b$表示的相对于$w_j$和$b$偏导数分别的输出， 虽然上面的表达式看起来很复杂，但有所有的偏导数，它实际上说的是非常简单的东西（这是个好消息）：$Δoutput$是权重和偏差中关于$Δw_j$和$Δb$的线性函数。这种线性使得选择权重和偏差的微小变化变得容易，以实现输出中任何所需的微小变化。因此，虽然 sigmoid 神经元与感知器具有许多相同的定性行为，但它们可以更容易地弄清楚改变权重和偏差将如何改变输出。</p>
<p>如果σ的形状真的很重要，而不是它的确切形式，那么为什么要使用在等式(3)中σ的特定形式？事实上，在本书的后面，我们偶尔会考虑神经元$f(w \cdot x + b)$ 对于其他一些激活函数$f(\cdot)$输出在哪里, 当我们使用不同的激活函数时，主要的变化是等式(5)中偏导数的特定值发生了变化。事实证明，当我们稍后计算这些偏导数时，使用σ将简化代数，仅仅是因为指数在微分时具有可爱的特性。在任何情况下，σ常用于神经网络的工作，也是我们在本书中最常使用的激活函数。</p>
<p>我们应该如何解释 sigmoid 神经元的输出？显然，感知器和 sigmoid 神经元之间的一大区别是 sigmoid 神经元不只是输出0和1. 它们可以输出0和1之间的任何实数, 所以值如0.173… 和0.689…是合法的输出。这可能很有用，例如，如果我们想使用输出值来表示输入到神经网络的图像中像素的平均强度。但有时它可能会令人讨厌。假设我们希望网络的输出指示“输入图像是 9”或“输入图像不是 9”。显然，如果输出是0或一个1，就像在感知器中一样。但在实践中，我们可以建立一个约定来处理这个问题，例如，决定解释至少0.5如指示“9”，并且任何输出小于0.5表示“不是9”。当我们使用这样的约定时，我总是会明确说明，所以它不应该引起任何混淆。</p>
<h2 id="神经网络架构"><a href="#神经网络架构" class="headerlink" title="神经网络架构"></a>神经网络架构</h2><p>在下一节中，我将介绍一个神经网络，它可以很好地对手写数字进行分类。为此，它有助于解释一些术语，这些术语可以让我们命名网络的不同部分。假设我们有网络：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/4.png" class="" title="网络">

<p>如前所述，这个网络中最左边的层称为输入层，层内的神经元称为输入神经元。最右边或输出层包含输出神经元，或者，在这种情况下，单个输出神经元。中间层称为 隐藏层，因为这一层的神经元既不是输入也不是输出。“隐藏”这个词可能听起来有点神秘——我第一次听到这个词时，我认为它一定有一些深刻的哲学或数学意义——但它的真正含义不过是“不是输入或输出”。上面的网络只有一个隐藏层，但有些网络有多个隐藏层。例如，下面的四层网络有两个隐藏层：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/5.png" class="" title="网络">

<p>有点令人困惑的是，由于历史原因，这种多层网络有时被称为多层感知器或 MLP，尽管它由 sigmoid 神经元组成，而不是感知器。我不会在本书中使用 MLP 术语，因为我认为它令人困惑，但我想警告你它的存在。</p>
<p>网络中输入和输出层的设计通常很简单。例如，假设我们正在尝试确定手写图像是否描绘了“9”。设计网络的一种自然方法是将图像像素的强度编码到输入神经元中。如果图像是64x64灰度图像，那么我们就有4,096&#x3D;64×64输入神经元，强度在0和1之间适度缩放。输出层将只包含一个神经元，输出值小于0.5表示“输入图像不是 9”，并且值大于0.5表示“输入图像是 9”。</p>
<p>虽然神经网络的输入和输出层的设计通常很简单，但隐藏层的设计却是一门艺术。特别是，不可能用一些简单的经验法则来总结隐藏层的设计过程。相反，神经网络研究人员为隐藏层开发了许多设计启发式方法，帮助人们从网络中获得他们想要的行为。例如，这种启发式方法可用于帮助确定如何权衡隐藏层的数量与训练网络所需的时间。我们将在本书后面遇到几个这样的设计启发式。</p>
<p>到目前为止，我们一直在讨论神经网络，其中一层的输出用作下一层的输入。这种网络称为前馈神经网络。这意味着网络中没有环路——信息总是前馈，从不反馈。如果我们确实有循环，我们最终会遇到<code>σ</code>函数的输入取决于输出的情况。这很难理解，所以我们不允许这样的循环。</p>
<p>然而，还有其他的人工神经网络模型，其中反馈回路是可能的。这些模型被称为 循环神经网络。这些模型中的想法是让神经元在有限的时间内激发，然后变为静止。这种放电可以刺激其他神经元，这些神经元可能会在一段时间后放电，而且持续时间有限。这会导致更多的神经元放电，因此随着时间的推移，我们会得到一连串的神经元放电。循环不会在这样的模型中引起问题，因为神经元的输出只会在稍后的某个时间影响其输入，而不是瞬间影响。</p>
<p>循环神经网络的影响力不如前馈网络，部分原因是循环网络的学习算法（至少迄今为止）没有那么强大。但是循环网络仍然非常有趣。它们在精神上比前馈网络更接近我们大脑的工作方式。并且循环网络有可能解决前馈网络解决起来非常困难的问题。然而，为了限制我们的范围，在本书中，我们将专注于更广泛使用的前馈网络。</p>
<h2 id="一个对手写数字进行分类的简单网络"><a href="#一个对手写数字进行分类的简单网络" class="headerlink" title="一个对手写数字进行分类的简单网络"></a>一个对手写数字进行分类的简单网络</h2><p>定义了神经网络之后，让我们回到手写识别。我们可以将识别手写数字的问题分成两个子问题。首先，我们想要一种方法将包含许多数字的图像分解为一系列单独的图像，每个图像都包含一个数字。例如，我们想破坏图像</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/6.png" class="" title="数字">

<!-- more -->

<p>分成六个独立的图像，</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/digits_separate.png" class="" title="分离">

<p>我们人类很容易解决这个分割问题，但是对于计算机程序来说，正确地分解图像是一个挑战。一旦图像被分割，程序就需要对每个单独的数字进行分类。因此，例如，我们希望我们的程序能够识别上面的第一个数字，</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_first_digit.png" class="" width="5">

<p>是 5。</p>
<p>我们将专注于编写一个程序来解决第二个问题，即对单个数字进行分类。我们这样做是因为事实证明分割问题并不难解决，只要你有一种对单个数字进行分类的好方法。有很多方法可以解决分割问题。一种方法是尝试多种不同的图像分割方式，使用单个数字分类器对每个试验分割进行评分。如果单个数字分类器对其在所有段中的分类有信心，则试验分割获得高分，如果分类器在一个或多个段中遇到很多问题，则得分低。这个想法是，如果分类器在某个地方遇到问题，那么它可能有问题，因为分割选择不正确。这个想法和其他变体可以很好地解决分割问题。因此，与其担心分割，我们将专注于开发一个可以解决更有趣和更困难的问题的神经网络，即识别单个手写数字。</p>
<p>为了识别单个数字，我们将使用三层神经网络：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/6.png" class="" title="网络">

<p>网络的输入层包含对输入像素值进行编码的神经元。正如下一节所讨论的，我们的网络训练数据将包含许多28x28扫描的手写数字的像素图像，因此输入层包含784&#x3D;28×28神经元。为简单起见，上图中我省略了大部分输入神经元。输入像素是灰度的，值为0.0代表白色，值为1.0 代表黑色，在代表逐渐变暗的灰色阴影的值之间。</p>
<p>网络的第二层是隐藏层。我们将这个隐藏层中的神经元数量表示为<code>n</code>, 我们将尝试不同的值<code>n</code>. 所示示例说明了一个小的隐藏层，仅包含<code>n = 15</code>神经元。</p>
<p>网络的输出层包含 <code>10</code> 个神经元。如果第一个神经元触发，即有一个输出≈1，那么这将表明网络认为该数字是 <code>0</code>. 如果第二个神经元触发，则表明网络认为该数字是 <code>1</code>. 等等。更准确地说，我们将输出神经元从<code>0</code>到<code>9</code>，并找出哪个神经元具有最高的激活值。如果那个神经元是，比如说，神经元数<code>6</code>，那么我们的网络会猜测输入数字是<code>6</code>. 对于其他输出神经元，依此类推。</p>
<p>你可能想知道为什么我们使用<code>10</code>输出神经元。毕竟，网络的目标是告诉我们哪个数字<code>(0,1,2,…,9)</code>对应于输入图像。一种看似自然的方式就是使用<code>4</code>输出神经元，将每个神经元视为取二进制值，取决于神经元的输出是否更接近 <code>0</code>或者<code>1</code>. 四个神经元足以编码答案，因为$2^4 &#x3D; 16$大于输入数字的 <code>10</code> 个可能值。为什么我们的网络要使用<code>10</code>代替神经元？这不是低效吗？最终的理由是经验性的：我们可以尝试两种网络设计，结果证明，对于这个特定问题，具有<code>10</code>输出神经元学会比网络更好地识别数字<code>4</code>输出神经元。但这让我们想知道为什么要使用<code>10</code>输出神经元效果更好。是否有一些启发式方法可以提前告诉我们应该使用<code>10-输出编码</code>而不是<code>4-输出编码</code>？</p>
<p>为了理解我们为什么要这样做，从第一原理开始思考神经网络在做什么是有帮助的。首先考虑我们使用的情况<code>10输出神经元</code>。让我们专注于第一个输出神经元，它试图确定数字是否为 <code>0</code>. 它通过权衡来自神经元隐藏层的证据来做到这一点。那些隐藏的神经元在做什么？好吧，为了论证起见，假设隐藏层中的第一个神经元检测是否存在如下图像：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/7.png" class="" title="图像">

<p>它可以通过对与图像重叠的输入像素进行大量加权来做到这一点，并且只对其他输入进行轻微加权。以类似的方式，为了论证，我们假设隐藏层中的第二、第三和第四个神经元检测是否存在以下图像：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/8.png" class="" title="图像">

<p>您可能已经猜到了，这四张图片共同构成了0我们在前面 显示的数字行中看到的图像：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_complete_zero.png" class="" title="图像">

<p>因此，如果所有四个隐藏的神经元都在放电，那么我们可以得出结论，这个数字是<code>0</code>. 当然，这不是我们可以用来断定该图像是<code>0</code>的唯一方式，还有许多其他方式（例如，通过上述图像的翻译或轻微扭曲）。但似乎可以肯定地说，至少在这种情况下，我们会得出结论，输入是<code>0</code>。</p>
<p>假设神经网络以这种方式运行，我们可以给出一个合理的解释来解释为什么最好有<code>10</code>来自网络的输出，而不是<code>4</code>. 如果我们有 <code>4</code> 输出，然后第一个输出神经元将尝试确定数字的最高有效位是什么。并且没有简单的方法可以将最重要的位与上面所示的简单形状相关联。很难想象有什么好的历史原因使数字的组件形状将与输出中最重要的位密切相关。</p>
<p>现在，尽管如此，这只是一个启发式。没有什么说三层神经网络必须以我描述的方式运行，隐藏的神经元检测简单的组件形状。也许一个聪明的学习算法会找到一些权重分配，让我们只使用<code>4</code>输出神经元。但作为一种启发式思维方式，我所描述的思维方式非常有效，并且可以为您节省大量时间来设计良好的神经网络架构。</p>
<h2 id="梯度下降学习"><a href="#梯度下降学习" class="headerlink" title="梯度下降学习"></a>梯度下降学习</h2><p>既然我们已经设计好了神经网络，那么它如何学会识别数字呢？我们首先需要的是一个可供学习的数据集——所谓的训练数据集。我们将使用 MNIST 数据集，其中包含数以万计的手写数字扫描图像及其正确分类。MNIST 的名称来源于它是 美国国家标准与技术研究院NIST收集的两个数据集的修改子集。以下是来自 MNIST 的一些图片：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/9.png" class="" title="图像">

<p>正如你所看到的，这些数字实际上与本章开头显示的数字相同，作为识别挑战。当然，在测试我们的网络时，我们会要求它识别不在训练集中的图像！</p>
<p>MNIST 数据分为两部分。第一部分包含 <code>60,000</code> 张图像，用作训练数据。这些图像是来自 <code>250</code> 人的扫描笔迹样本，其中一半是美国人口普查局员工，其中一半是高中生。这些图像是灰度的，大小为 <code>28 x 28</code> 像素。MNIST 数据集的第二部分是 <code>10,000</code> 张图像用作测试数据。同样，这些是 <code>28 x 28</code> 灰度图像。我们将使用测试数据来评估我们的神经网络学习识别数字的能力。为了使这成为一个良好的性能测试，测试数据取自不同的比原始训练数据多 <code>250</code> 人（尽管仍然是人口普查局员工和高中生之间的分组）。这有助于让我们相信我们的系统可以识别出训练期间没有看到的人的数字。</p>
<p>我们将使用符号$x$表示训练输入。把每个训练输入作为<code>28×28=784</code>维向量会很方便。向量中的每个条目代表图像中单个像素的灰度值。我们将相应的期望输出表示为$y &#x3D; y(x)$， $y$是一个<code>10</code>维向量。例如，如果一个特定的训练图像，$x$，描绘了一个<code>6</code>， 然后$y(x) &#x3D; (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T $ 是网络的期望输出。注意$T$是转置操作，将行向量转换为普通（列）向量。</p>
<p>我们想要的是一种算法，它可以让我们找到权重和偏差，以便对于所有训练输入$x$网络的输出接近$y&#x3D;f(x)$. 为了量化我们实现这一目标的程度，我们定义了一个成本函数:</p>
<p>\begin{eqnarray}  C(w,b) \equiv<br>  \frac{1}{2n} \sum_x | y(x) - a|^2.<br>\tag{6}\end{eqnarray}</p>
<p>这里，$w$表示网络中所有权重的集合，$b$表示所有的偏差，$n$是训练输入的总数，当$x$是输入，$a$是网络输出的向量。$sum$是所有训练输入$x$的总和. 当然，输出$a$取决于$x$,$w$和$b$，但为了保持符号简单，我没有明确指出这种依赖关系。符号$| v |$只是表示向量$v$的通常长度函数. 我们把$C$叫做二次成本函数；它有时也称为均方差或简称为<code>MSE</code>。检查二次成本函数的形式，我们看到$C(w,b)$是非负的，因为总和中的每一项都是非负的。此外，对于所有训练输入$x$来说当$y(x)$大约等于输出$a$时成本函数$C(w,b)$变小，即$C(w,b) \approx 0$。 所以我们的训练算法如果可以找到权重和偏差致使$C(w,b) \approx 0$ 那么这个算法就做的很好了。相比之下，当$C(w,b)$很大时 - 这意味着对于大量的输入$y(x)$不接近输出$a$，那么算法就做的不好了。所以我们的训练算法的目标是最小化作为权重和偏差的函数$C(w,b)$。换句话说，我们希望找到一组使成本尽可能小的权重和偏差。我们将使用一种称为梯度下降的算法来做到这一点。</p>
<p>为什么要引入二次成本？毕竟，我们不是主要对网络正确分类的图像数量感兴趣吗？为什么不尝试直接最大化该数字，而不是最小化二次成本之类的代理度量？问题在于正确分类的图像数量不是网络中权重和偏差的平滑函数。在大多数情况下，对权重和偏差进行小的更改不会导致正确分类的训练图像数量发生任何变化。这使得很难弄清楚如何改变权重和偏差来提高性能。如果我们改为使用像二次成本这样的平滑成本函数，那么很容易弄清楚如何对权重和偏差进行微小的改变，从而改进成本。</p>
<p>即使我们想要使用平滑成本函数，您可能仍然想知道为什么我们选择方程<code>(6)</code>中使用的二次函数。这不是一个相当临时的选择吗？也许如果我们选择不同的成本函数，我们会得到一组完全不同的最小化权重和偏差？这是一个有效的关注点，稍后我们将重新审视成本函数，并进行一些修改。然而，等式<code>(6)</code>的二次成本函数非常适合理解神经网络学习的基础知识，所以我们现在将坚持使用它。</p>
<p>回顾一下，我们训练神经网络的目标是找到最小化二次成本函数$C(w,b)$的权重和偏差. 这是一个提法恰当的问题，但它有很多目前提出的困扰的结构——对$w$和$b$作为权重和偏差的解释，潜伏在后台的$σ$函数、网络架构的选择、MNIST等。<em><strong>事实证明，我们可以通过忽略大部分结构来理解大量内容，而只关注最小化方面</strong></em>。所以现在我们要忘记成本函数的具体形式、与神经网络的连接等等。相反，我们将想象我们只是简单地获得了一个包含许多变量的函数，并且我们希望最小化该函数。我们将开发一种称为梯度下降的技术，可用于解决此类最小化问题。然后我们将回到最小化的特定函数。</p>
<p>好的，假设我们试图最小化某个函数$C(v)$. 这可以是许多变量的任何实值函数，$v &#x3D;v1,v2,…$. 请注意，我已经用$v$替换了$w$和$b$ 强调这可以是任何函数——我们不再专门考虑神经网络上下文。最小化$C(v)$，把$C$作为两个变量的函数是有助于想象的:</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/valley.png" class="" title="valley">

<p>我们想要的是找到$C$在哪里达到其全局最小值。现在，当然，对于上面绘制的函数，我们可以观察图表并找到最小值。从这个意义上说，我可能展示了一个过于简单的函数！一个通用函数$C$可能是一个包含许多变量的复杂函数，通常不可能仅仅通过观察图形来找到最小值。</p>
<p>解决问题的一种方法是使用微积分来尝试分析找到最小值。我们可以计算导数，然后尝试使用它们来找到$C$极值的位置。如果$C$是一个或几个变量的函数运气好的话可能会起作用。但是当我们有更多的变量时，它会变成一场噩梦。对于神经网络，我们通常需要更多的变量——最大的神经网络的成本函数（它以极其复杂的方式依赖于数十亿的权重和偏差）。使用微积分将其最小化是行不通的！</p>
<p>好吧，所以微积分不起作用。幸运的是，有一个漂亮的类比暗示了一种运行良好的算法。我们首先将我们的功能视为一种山谷。如果您对上面的情节稍微眯眼，那应该不会太难。我们想象一个球从山谷的斜坡上滚下来。我们的日常经验告诉我们，球最终会滚到谷底。也许我们可以使用这个想法来找到函数的最小值？我们会随机选择一个（假想的）球的起点，然后模拟球滚到谷底时的运动。我们可以简单地通过计算$C$的导数（可能还有一些二阶导数）来进行模拟，这些导数会告诉我们关于山谷的当地“形状”以及我们的球应该如何滚动的所有信息。</p>
<p>根据我刚刚写的内容，您可能会认为我们将尝试写下球的牛顿运动方程，考虑摩擦和重力的影响等等。实际上，我们不会那么认真地对待滚球类比——我们正在设计一种算法来最小化$C$，而不是开发物理定律的精确模拟！球的视角旨在激发我们的想象力，而不是限制我们的思维。因此，与其深入研究物理学的所有混乱细节，不如问问自己：如果我们被宣布为上帝一天，并且可以制定我们自己的物理定律，告诉球它应该如何滚动，什么定律或定律我们是否可以选择这样的运动，使球总是滚到谷底？</p>
<p>为了使这个问题更准确，让我们考虑一下当我们将球$v1$方向上移动$Δv1$时会发生什么，然后在$v2$方向上移动$Δv2$。微积分告诉我们$C$变化如下：</p>
<p>\begin{eqnarray}<br>  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +<br>  \frac{\partial C}{\partial v_2} \Delta v_2<br>\tag{7}\end{eqnarray}</p>
<p>我们会找到一种选择$Δv1$和$Δv2$的方式从而使$ΔC$为负; 即，我们将选择它们，使球滚入山谷。为了弄清楚如何做出这样的选择，它有助于定义$Δv$成为变化的向量$v$ $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$， $T$又是转置操作，将行向量变成列向量。我们还将定义梯度$C$ 为偏导数的向量 $\left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T$. 我们将梯度向量表示为$∇C$：</p>
<p>\begin{eqnarray}<br>  \nabla C \equiv \left( \frac{\partial C}{\partial v_1},<br>  \frac{\partial C}{\partial v_2} \right)^T<br>\tag{8}\end{eqnarray}</p>
<p>稍后我们将按照$Δv$和梯度$∇C$重写更改$ΔC$. 不过，在开始之前，我想澄清一些有时会让人们挂在梯度的东西。第一次使用$∇C$符号，人们有时想知道他们应该如何考虑$∇$符号。$∇$的确切含义是什么？$∇C$作为一个单一的数学对象其实想想也挺好的——上面定义的向量——恰好是用两个符号写的。从这个角度来看，$∇$只是一个符号，告诉你“嘿，$∇C$是一个梯度向量”。还有更高级的观点，其中$∇$可以将其视为一个独立的数学实体（例如，作为微分算子），但我们不需要这样的观点。</p>
<p>有了这些定义，表达式(7)为 $ΔC$可以改写为:</p>
<p>\begin{eqnarray}<br>  \Delta C \approx \nabla C \cdot \Delta v<br>\tag{9}\end{eqnarray}</p>
<p>这个等式有助于解释为什么$∇C$称为梯度向量：$∇C$将$v$的变化与$C$的变化联系起来，就像我们期望的那样，叫做渐变。但这个等式真正令人兴奋的是它让我们看到了如何选择$Δv$从而使$ΔC$为负。特别是，假设我们选择：</p>
<p>\begin{eqnarray}<br>  \Delta v &#x3D; -\eta \nabla C<br>\tag{10}\end{eqnarray}</p>
<p>$η$是一个小的正参数（称为 学习率）。然后等式（9）告诉我们$\Delta C \approx -\eta \nabla C \cdot \nabla C &#x3D; -\eta |\nabla C|^2$. 因为$| \nabla C|^2 \geq 0$，这保证了$\Delta C \leq 0$，比如，如果我们通过（10）的公式来改变$v$，$C$将总是减少，不会增加。（当然，在等式(9)的近似值范围内）。这正是我们想要的属性！因此，我们将采用等式(10)来定义梯度下降算法中球的“运动定律”。也就是说，我们将使用等式(10)来给$Δv$计算一个数值，然后用这个数字来移动球的位置$v$：</p>
<p>\begin{eqnarray}<br>  v \rightarrow v’ &#x3D; v -\eta \nabla C.<br>\tag{11}\end{eqnarray}</p>
<p>然后我们将再次使用此更新规则，进行另一次移动。如果我们继续这样做，一遍又一遍，我们将继续减少$C$直到——我们希望——我们达到最低限度。</p>
<p>总结一下，梯度下降算法的工作方式就是重复计算梯度$∇C$，然后向相反的方向移动，“落下”山谷的斜坡。我们可以像这样可视化它：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/valley_with_ball.png" class="" title="valley with ball">

<p>请注意，使用此规则梯度下降不会再现真实的物理运动。在现实生活中，一个球有动量，而这种动量可能让它滚过斜坡，甚至（暂时）滚上坡。只有在摩擦力作用后，球才能保证滚落到山谷中。相比之下，我们的选择规则$Δv$只是说“现在就下去”。这仍然是找到最小值的一个很好的规则！</p>
<p>为了使梯度下降正确工作，我们需要选择学习率这足够小，以至于等式（9）是一个很好的近似值。如果我们不这样做，我们最终可能会得到$\Delta C &gt; 0$，这显然不好！同时，我们不想这太小了，因为那会做出改变$Δv$很小，因此梯度下降算法会运行得很慢。在实际实现中，这经常变化，因此等式(9)仍然是一个很好的近似值，但算法并不太慢。我们稍后会看到这是如何工作的。</p>
<p>我已经解释了$C$是只有两个变量的函数的梯度下降。但是，事实上，即使在$C$是更多变量的函数也能很好的工作。特别假设$C$是一个含有$m$个变量的函数时，$v_1,…,v_m$. 然后$ΔC$由$C$微小的变化产生$\Delta v &#x3D; (\Delta v_1,\ldots, \Delta v_m)^T$是:</p>
<p>\begin{eqnarray}<br>  \Delta C \approx \nabla C \cdot \Delta v<br>\tag{12}\end{eqnarray}</p>
<p>梯度$∇C$是向量:</p>
<p>\begin{eqnarray}<br>  \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots,<br>  \frac{\partial C}{\partial v_m}\right)^T<br>\tag{13}\end{eqnarray}</p>
<p>就像两个变量的情况一样，我们可以选择：</p>
<p>\begin{eqnarray}<br>  \Delta v &#x3D; -\eta \nabla C<br>\tag{14}\end{eqnarray}</p>
<p>并且我们保证我们的（近似）表达式（12）中$ΔC$将是负的。这为我们提供了一种即使在$C$是许多变量的函数将梯度最小化的方法，通过重复应用更新规则：</p>
<p>\begin{eqnarray}<br>  v \rightarrow v’ &#x3D; v-\eta \nabla C<br>\tag{15}\end{eqnarray}</p>
<p>您可以将此更新规则视为定义梯度下降算法。它为我们提供了一种反复改变位置$v$的方法来找到函数$C$的最小值. 该规则并不总是有效 - 一些事情可能会出错并阻止梯度下降找到$C$的全局最小值，我们将在后面的章节中再次探讨这一点。但是，在实践中梯度下降通常效果非常好，在神经网络中，我们会发现它是一种最小化成本函数的强大方法，因此有助于网络学习。</p>
<p>事实上，甚至在某种意义上，梯度下降是寻找最小值的最佳策略。假设我们正在尝试移动位置$Δv$，以减少$C$越多越好。这相当于最小化$\Delta C \approx \nabla C \cdot \Delta v$. 我们将限制移动的大小，以便对于一些小固定$\epsilon &gt; 0$来说$|\Delta v | &#x3D; \epsilon$. 换句话说，我们想要一个固定大小的小步的移动，并且我们试图找到减小$C$越多越好的移动方向。可以证明，选择$Δv$最小化$\Delta v &#x3D; - \eta \nabla C$， 当$\eta &#x3D; \epsilon &#x2F; |\nabla C|$由$|\Delta v| &#x3D; \epsilon$大小约束决定. 所以梯度下降可以被视为一种在最能立即减少$C$的方向上采取小步骤的方法。</p>
<p>人们已经研究了梯度下降的许多变体，包括更接近模拟真实物理球的变体。这些模拟球的变化有一些优点，但也有一个主要缺点：结果证明有必要计算函数$C$的二阶偏导数，这可能会非常昂贵。要了解它为何如此昂贵，假设我们要计算所有的二阶偏导数$\partial^2 C&#x2F; \partial v_j \partial v_k$. 如果有一百万像$v_j$这样的变量然后我们需要计算类似一万亿的二阶偏导数,这将是计算成本高昂的。话虽如此，有一些技巧可以避免这种问题，寻找梯度下降的替代方案是一个活跃的研究领域。但在本书中，我们将使用梯度下降作为我们在神经网络中学习的主要方法。</p>
<p>我们如何应用梯度下降来学习神经网络？这个想法是使用梯度下降来找到权重$w_k$和偏差$b_l$（等式(6)中的最小化成本）。为了看看它是如何工作的，让我们重述梯度下降更新规则，用权重和偏差替换变量$v_j$. 换句话说，我们的“位置”现在有了组件$w_k$和$b_l$, 和梯度向量$\nabla C$有对应的组件$\partial C &#x2F; \partial w_k$在到$\partial C&#x2F; \partial b_l$. 根据分量写出梯度下降更新规则，我们有:</p>
<p>\begin{eqnarray}<br>  w_k &amp; \rightarrow &amp; w_k’ &#x3D; w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\<br>  b_l &amp; \rightarrow &amp; b_l’ &#x3D; b_l-\eta \frac{\partial C}{\partial b_l}<br>\tag{17}\end{eqnarray}</p>
<p>通过重复应用这个更新规则，我们可以“滚下山”，并希望找到成本函数的最小值。换句话说，这是一个可用于在神经网络中学习的规则。</p>
<p>应用梯度下降规则存在许多挑战。我们将在后面的章节中深入研究这些内容。但现在我只想提一个问题。要了解问题所在，让我们回顾一下方程式(6)中的二次成本。请注意，此成本函数具有以下形式$C &#x3D; \frac{1}{n} \sum_x C_x$，也就是说，它是超过成本的平均值$C_x \equiv \frac{|y(x)-a|^2}{2}$ -用于个人训练示例。在实践中，为了计算梯度$\nabla C$我们需要分别为每个训练输入$x$计算梯度$\nabla C_x$，然后平均它们，$\nabla C &#x3D; \frac{1}{n} \sum_x \nabla C_x$. 不幸的是，当训练输入的数量非常大时，这可能需要很长时间，因此学习速度很慢。</p>
<p>一种称为<em><strong>随机梯度下降</strong></em>的想法可用于加速学习。这个想法是通过对于随机选择的训练输入的小样本计算$\nabla C_x$来估计梯度$\nabla C$。通过对这个小样本进行平均，事实证明我们可以快速获得对真实梯度$\nabla C$的良好估计，这有助于加速梯度下降，从而加速学习。</p>
<p>为了使这些想法更精确，随机梯度下降通过从随机选择的训练输入中随机挑选一个小数$m$来。我们将标记那些随机的训练输入$x_1,x_2,…,x_m$，并将它们称为mini-batch。提供样本量$m$足够大，我们期望$\nabla C_{X_j}$将大致等于全部平均值$\nabla C_x$， 那是：</p>
<p>\begin{eqnarray}<br>  \frac{\sum_{j&#x3D;1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} &#x3D; \nabla C<br>\tag{18}\end{eqnarray}</p>
<p>其中第二个总和是整个训练数据集。左右交换后我们得到：</p>
<p>\begin{eqnarray}<br>  \nabla C \approx \frac{1}{m} \sum_{j&#x3D;1}^m \nabla C_{X_{j}},<br>\tag{19}\end{eqnarray}</p>
<p>可以确定的是我们可以通过计算随机选择的小批量的梯度来估计整体梯度。</p>
<p>为了明确地将其与神经网络中的学习联系起来，假设$w_k$和$b_l$表示我们神经网络中的权重和偏差。然后随机梯度下降通过挑选一个随机选择的小批量训练输入来工作，并用这些来训练：</p>
<p>\begin{eqnarray}<br>  w_k &amp; \rightarrow &amp; w_k’ &#x3D; w_k-\frac{\eta}{m}<br>  \sum_j \frac{\partial C_{X_j}}{\partial w_k}<br>  \tag{20}\end{eqnarray}</p>
<p>\begin{eqnarray}<br>  b_l &amp; \rightarrow &amp; b_l’ &#x3D; b_l-\frac{\eta}{m}<br>  \sum_j \frac{\partial C_{X_j}}{\partial b_l}<br>\tag{21}\end{eqnarray}</p>
<p>其中总和是所有训练样本$X_j$在当前的小批量中的总和。然后我们挑选另一个随机选择的 mini-batch 并用它们进行训练。以此类推，直到我们用尽了训练输入，据说这完成了一个训练<em><strong>周期</strong></em>。在那一点上，我们从一个新的训练时期重新开始。</p>
<p>顺便说一句，值得注意的是，关于成本函数的缩放以及对权重和偏差的小批量更新的约定有所不同。在等式（6）中，我们将整体成本函数按一个因子缩放$\frac{1}{n}$， 人们有时会忽略$\frac{1}{n}$，对单个训练样本的成本求和而不是平均。当事先不知道训练样本的总数时，这特别有用。例如，如果实时生成更多训练数据，就会发生这种情况。而且，以类似的方式，小批量更新规则(20) 和(21)有时会省略算出总和前面的$\frac{1}{m}$。从概念上讲，这几乎没有什么区别，因为它相当于重新调整学习率$\eta$. 但是，在对不同工作进行详细比较时，值得提防。</p>
<p>我们可以将随机梯度下降视为政治投票：对小批量进行抽样比对整个批量应用梯度下降要容易得多，就像进行投票比进行完整选举更容易一样。例如，如果我们有一个大小为$n &#x3D; 60,000$的训练集，就像在 MNIST 中一样，并选择一个 mini-batch 大小（比如说）$m &#x3D; 10$，这意味着我们将得到一个因子$6,000$加速估计梯度！当然，估计不会是完美的 - 会有统计波动 - 但它不需要完美：我们真正关心的是朝着有助于减少$C$的总体方向前进，这意味着我们不需要精确计算梯度。在实践中，随机梯度下降是神经网络学习中常用且强大的技术，它是我们将在本书中开发的大多数学习技术的基础。</p>
<p>让我通过讨论一个有时会困扰刚接触梯度下降的人的观点来结束本节。在神经网络中，成本$C$是许多变量的函数——所有的权重和偏差——因此在某种意义上定义了一个非常高维空间中的表面。有些人会挂断电话：“嘿，我必须能够可视化所有这些额外的维度”。他们可能会开始担心：“我无法在四个维度上思考，更不用说五个（或五百万）了”。他们是否缺少某些“真正的”超数学家拥有的某些特殊能力？当然，答案是否定的。即使是最专业的数学家也无法很好地可视化四个维度，如果有的话。相反，他们使用的技巧是开发其他方式来表示正在发生的事情。这正是我们上面所做的：我们使用代数（而不是视觉）表示$\Delta C$，弄清楚如何移动以减少$C$. 擅长高维度思考的人有一个头脑中的图书馆，其中包含许多不同的技术。我们的代数技巧只是一个例子。这些技术可能没有我们在可视化三个维度时所习惯的简单性，但是一旦您建立了此类技术的库，您就可以很好地进行高维度的思考。我不会在这里详细介绍，但是如果您有兴趣，那么您可能会喜欢阅读 有关专业数学家在高维中思考的一些技术的讨论。虽然讨论的一些技术相当复杂，但大部分最好的内容都是直观且易于访问的，任何人都可以掌握。</p>
<h2 id="实现我们的网络对数字进行分类"><a href="#实现我们的网络对数字进行分类" class="headerlink" title="实现我们的网络对数字进行分类"></a>实现我们的网络对数字进行分类</h2><p>好的，让我们编写一个程序，使用随机梯度下降和 MNIST 训练数据来学习如何识别手写数字。我们将使用一个简短的 Python (2.7) 程序来做到这一点，只有 74 行代码！我们需要做的第一件事是获取 MNIST 数据。如果你是git用户，那么你可以通过克隆本书的代码库来获取数据，</p>
<pre><code>git 克隆 https://github.com/mnielsen/neural-networks-and-deep-learning.git
</code></pre>
<!-- more -->
<p>顺便说一句，当我之前描述 MNIST 数据时，我说它被分成 60,000 个训练图像和 10,000 个测试图像。这是官方的 MNIST 描述。实际上，我们将对数据进行稍微不同的拆分。我们将保持测试图像不变，但将 60,000 张图像的 MNIST 训练集分为两部分：一组 50,000 张图像，我们将用于训练我们的神经网络，以及一个单独的 10,000 张图像验证集。我们不会在本章中使用验证数据，但在本书后面我们会发现它在弄清楚如何设置某些 超参数时很有用神经网络的 - 诸如学习率之类的东西，这些不是我们的学习算法直接选择的。尽管验证数据不是原始 MNIST 规范的一部分，但许多人以这种方式使用 MNIST，并且验证数据的使用在神经网络中很常见。从现在开始，当我提到“MNIST 训练数据”时，我指的是我们的 50,000 张图像数据集，而不是原始的 60,000 张图像数据集。</p>
<p>除了 MNIST 数据，我们还需要一个名为 Numpy的 Python 库，用于进行快速线性代数。如果你还没有安装 Numpy，你可以在 这里得到它。</p>
<p>在给出完整列表之前，让我解释一下神经网络代码的核心特征。核心是一个网络 类，我们用它来表示一个神经网络。下面是我们用来初始化Network对象的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sizes</span>):</span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x) </span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[:-<span class="number">1</span>], sizes[<span class="number">1</span>:])]</span><br></pre></td></tr></table></figure>

<p>在此代码中，sizes大小包含各个层中的神经元数量。因此，例如，如果我们想创建一个<code>Network</code>对象，第一层有 2 个神经元，第二层有 3 个神经元，最后一层有 1 个神经元，我们将使用以下代码执行此操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = Network([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>Network对象中的偏差和权重都是随机初始化的，使用Numpy的<code>np.random.randn</code>函数生成均值的高斯分布0和标准差1. 这种随机初始化为我们的随机梯度下降算法提供了一个起点。在后面的章节中，我们会找到更好的方法来初始化权重和偏差，但现在就可以了。请注意，网络初始化代码假定第一层神经元是输入层，并省略为这些神经元设置任何偏差，因为偏差仅用于计算后面层的输出。</p>
<p>另请注意，偏差和权重存储为 Numpy 矩阵列表。因此，例如net.weights[1]是一个 Numpy 矩阵，用于存储连接第二层和第三层神经元的权重。（它不是第一层和第二层，因为 Python 的列表索引从0开始。）由于<code>net.weights[1]</code>相当冗长，我们只表示该矩阵$w$. 这是一个矩阵，使$w_{jk}$表示第二层的神经元$k^{\rm th}$，和第三层神经元$j^{\rm th}$之间的连接。这个$j$ $k$排序的可能看起来很奇怪 - 当然，交换$j$和$k$索引会更有意义？使用这种排序的最大优点是它意味着第三层神经元的激活向量是：</p>
<p>\begin{eqnarray}<br>  a’ &#x3D; \sigma(w a + b).<br>\tag{22}\end{eqnarray}</p>
<p>这个等式有很多东西，所以让我们一块一块地解开它。 $a$是第二层神经元的激活向量。为了获得$a’$我们用权重矩阵$w$与$a$相乘, 并加上偏差向量$b$。然后我们应用函数$σ$ 向量中每个条目的元素$wa + b$. （这称为 向量化函数 σ.) 用于计算 sigmoid 神经元的输出, 很容易验证公式(22)给出的结果与我们之前的规则公式(4)相同。</p>
<p>考虑到这一切，编写代码计算网络实例的输出很容易。我们首先定义 sigmoid 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br></pre></td></tr></table></figure>

<p>请注意，当输入<code>z</code>是向量或 Numpy 数组时，Numpy 会自动按元素应用函数sigmoid，即以向量化形式。然后我们向Network类添加一个<code>feedforward</code>方法，给定网络的输入<code>a</code> ，该方法返回相应的输出, 该方法所做的只是对每一层 应用公式(22)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, a</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the output of the network if &quot;a&quot; is input.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">        a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<p>当然，我们希望Network对象做的主要事情是学习。为此，我们将给他们一个实现随机梯度下降的<code>SGD</code>方法。这是代码。在某些地方有点神秘，但我会在下面列出之后将其分解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">        test_data=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">    gradient descent.  The &quot;training_data&quot; is a list of tuples</span></span><br><span class="line"><span class="string">    &quot;(x, y)&quot; representing the training inputs and the desired</span></span><br><span class="line"><span class="string">    outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">    self-explanatory.  If &quot;test_data&quot; is provided then the</span></span><br><span class="line"><span class="string">    network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">    epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">    tracking progress, but slows things down substantially.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> test_data: n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">    n = <span class="built_in">len</span>(training_data)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">        random.shuffle(training_data)</span><br><span class="line">        mini_batches = [</span><br><span class="line">            training_data[k:k+mini_batch_size]</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">            self.update_mini_batch(mini_batch, eta)</span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                j, self.evaluate(test_data), n_test)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125; complete&quot;</span>.<span class="built_in">format</span>(j)</span><br></pre></td></tr></table></figure>
<p><code>training_data</code>是 表示训练输入和相应期望输出的元组<code>(x, y)</code>的列表。变量<code>epochs</code>和<code>mini_batch_size</code>是您所期望的 - 要训练的 <code>epoch</code> 数，以及采样时要使用的小批量的大小。 $\eta$是学习率 如果提供了可选参数<code>test_data</code>，那么程序将在每个训练阶段后评估网络，并打印出部分进度。这对于跟踪进度很有用，但会大大减慢速度。</p>
<p>该代码的工作原理如下。在每个 <code>epoch</code> 中，它首先随机打乱训练数据，然后将其划分为适当大小的 <code>mini-batch</code>。这是一种从训练数据中随机抽样的简单方法。然后对于每个<code>mini_batch</code>，我们应用一步梯度下降。这是由代码 <code>self.update_mini_batch(mini_batch, eta)</code>完成的，它根据梯度下降的单次迭代更新网络权重和偏差，仅使用<code>mini_batch</code>中的训练数据。以下是<code>update_mini_batch</code>方法的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Update the network&#x27;s weights and biases by applying</span></span><br><span class="line"><span class="string">    gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">    The &quot;mini_batch&quot; is a list of tuples &quot;(x, y)&quot;, and &quot;eta&quot;</span></span><br><span class="line"><span class="string">    is the learning rate.&quot;&quot;&quot;</span></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">        delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">        nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">        nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">    self.weights = [w-(eta/<span class="built_in">len</span>(mini_batch))*nw </span><br><span class="line">                    <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(self.weights, nabla_w)]</span><br><span class="line">    self.biases = [b-(eta/<span class="built_in">len</span>(mini_batch))*nb </span><br><span class="line">                   <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure>
<p>大部分工作由下面这行来完成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br></pre></td></tr></table></figure>
<p>这调用了一种称为反向传播算法的东西，这是一种计算成本函数梯度的快速方法。因此<code>update_mini_batch</code>只需为<code>mini_batch</code>中的每个训练示例计算这些梯度，然后适当地更新 <code>self.weights和self.biases</code> 即可。</p>
<p>我现在不打算展示<code>self.backprop</code>的代码。我们将在下一章研究反向传播的工作原理，包括<code>self.backprop</code>的代码。现在，只需假设它的行为与声明的一样，返回与训练示例<code>x</code>相关的成本的适当梯度。</p>
<p>让我们看看完整的程序，包括我上面省略的文档字符串。除了<code>self.backprop</code>程序是不言自明的——所有繁重的工作都是在<code>self.SGD</code> 和<code>self.update_mini_batch</code>中完成的，我们已经讨论过了。<code>self.backprop</code> 方法使用了一些额外的函数来帮助计算梯度，即<code>sigmoid_prime</code>，它计算$σ$函数，以及 <code>self.cost_derivative</code>，这里我不会描述。只需查看代码和文档字符串，您就可以了解这些要点（也许还有细节）。我们将在下一章详细介绍它们。请注意，虽然程序看起来很长，但大部分代码都是文档字符串，旨在使代码易于理解。事实上，该程序仅包含 <code>74</code> 行非空白、非注释代码。所有代码都可以在 <code>GitHub</code> 上 找到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">network.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A module to implement the stochastic gradient descent learning</span></span><br><span class="line"><span class="string">algorithm for a feedforward neural network.  Gradients are calculated</span></span><br><span class="line"><span class="string">using backpropagation.  Note that I have focused on making the code</span></span><br><span class="line"><span class="string">simple, easily readable, and easily modifiable.  It is not optimized,</span></span><br><span class="line"><span class="string">and omits many desirable features.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sizes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won&#x27;t set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers.&quot;&quot;&quot;</span></span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[:-<span class="number">1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            test_data=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125; complete&quot;</span>.<span class="built_in">format</span>(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Update the network&#x27;s weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate.&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/<span class="built_in">len</span>(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/<span class="built_in">len</span>(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``.&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[-<span class="number">1</span>])</span><br><span class="line">        nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It&#x27;s a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, test_data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network&#x27;s output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation.&quot;&quot;&quot;</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">int</span>(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cost_derivative</span>(<span class="params">self, output_activations, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>

<p>该程序识别手写数字的能力如何？好吧，让我们从加载 MNIST 数据开始。我将使用一个小帮助程序<code>mnist_loader.py</code>来完成此操作，如下所述。我们在 Python shell 中执行以下命令，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> mnist_loader</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = \</span><br><span class="line"><span class="meta">... </span>mnist_loader.load_data_wrapper()</span><br></pre></td></tr></table></figure>

<p>当然，这也可以在一个单独的 Python 程序中完成，但如果您遵循它可能最容易在 Python shell 中完成。<br>加载 MNIST 数据后，我们将建立一个<code>30</code>隐藏的神经元的网络。我们在导入上面列出的名为network的 Python 程序后执行此操作，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> network</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>最后，我们将使用随机梯度下降从 MNIST <code>training_data</code>中学习超过 30 个epoch，小批量大小为 10，学习率为$\eta &#x3D; 3.0$,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>请注意，如果您在阅读时运行代码，则需要一些时间来执行 - 对于典型的机器（截至 2015 年），可能需要几分钟才能运行。我建议您设置运行，继续阅读，并定期检查代码的输出。如果您赶时间，可以通过减少 epoch 的数量、减少隐藏神经元的数量或仅使用部分训练数据来加快速度。请注意，生产代码会快得多：这些 Python 脚本旨在帮助您了解神经网络的工作原理，而不是高性能代码！当然，一旦我们训练了一个网络，它就可以在几乎任何计算平台上快速运行。例如，一旦我们为网络学习了一组好的权重和偏差，它可以很容易地移植到 Web 浏览器中以 Javascript 运行，或作为移动设备上的本机应用程序运行。无论如何，这里是神经网络一次训练运行的输出的部分副本。成绩单显示了在每个训练阶段后神经网络正确识别的测试图像的数量。如您所见，仅在一个 epoch 之后，这一数字就达到了 <code>9,129/10,000</code>，而且这个数字还在继续增长，<br>    Epoch 0: 9129 &#x2F; 10000<br>    Epoch 1: 9295 &#x2F; 10000<br>    Epoch 2: 9348 &#x2F; 10000<br>    …<br>    Epoch 27: 9528 &#x2F; 10000<br>    Epoch 28: 9542 &#x2F; 10000<br>    Epoch 29: 9534 &#x2F; 10000</p>
<p>也就是说，经过训练的网络给我们的分类率约为 <code>95%</code> - <code>95.42%</code>最高（“28 epoch”）！作为第一次尝试，这非常令人鼓舞。但是，我应该警告您，如果您运行代码，那么您的结果不一定与我的结果完全相同，因为我们将使用（不同的）随机权重和偏差来初始化我们的网络。为了在本章中产生结果，我采取了三局两胜的方式。</p>
<p>让我们重新运行上面的实验，将隐藏神经元的数量更改为100. 与之前的情况一样，如果您在阅读时运行代码，则应该警告您需要相当长的时间才能执行（在我的机器上，这个实验每个训练 epoch 需要数十秒），所以明智的做法是在代码执行时继续并行阅读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">100</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>果然，这将结果提高到<code>96.59%</code>。至少在这种情况下，使用更多的隐藏神经元有助于我们获得更好的结果</p>
<p>当然，为了获得这些准确度，我必须对训练的 epoch 数、<code>mini-batch</code> 大小和学习率做出具体选择，这. 正如我上面提到的，这些被称为我们神经网络的超参数，以便将它们与我们的学习算法学习的参数（权重和偏差）区分开来。如果我们选择不当的超参数，我们可能会得到不好的结果。例如，假设我们选择了学习率$\eta &#x3D; 0.001$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">100</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">0.001</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>结果远没有那么令人鼓舞<br>    Epoch 0: 1139 &#x2F; 10000<br>    Epoch 1: 1136 &#x2F; 10000<br>    Epoch 2: 1135 &#x2F; 10000<br>    …<br>    Epoch 27: 2101 &#x2F; 10000<br>    Epoch 28: 2123 &#x2F; 10000<br>    Epoch 29: 2142 &#x2F; 10000</p>
<p>但是，您可以看到网络的性能随着时间的推移慢慢变得更好。这表明提高学习率，比如说$\eta &#x3D; 0.01$. 如果我们这样做，我们会得到更好的结果，这表明再次提高学习率。（如果做出改变可以改善事情，请尝试做更多！）如果我们多次这样做，我们最终会得到类似的学习率$\eta &#x3D; 1.0$（也许微调到<code>3.0</code>)，这与我们之前的实验很接近。因此，即使我们最初对超参数的选择很糟糕，但我们至少获得了足够的信息来帮助我们改进对超参数的选择。</p>
<p>一般来说，调试神经网络可能具有挑战性。当超参数的初始选择产生的结果不比随机噪声好时，尤其如此。假设我们尝试了之前成功的 <code>30</code> 个隐藏神经元网络架构，但是学习率变为$\eta &#x3D; 100.0$:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">100.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>在这一点上我们实际上已经走得太远了，学习率太高了：<br>    Epoch 0: 1009 &#x2F; 10000<br>    Epoch 1: 1009 &#x2F; 10000<br>    Epoch 2: 1009 &#x2F; 10000<br>    Epoch 3: 1009 &#x2F; 10000<br>    …<br>    Epoch 27: 982 &#x2F; 10000<br>    Epoch 28: 982 &#x2F; 10000<br>    Epoch 29: 982 &#x2F; 10000</p>
<p>现在想象一下，我们是第一次遇到这个问题。当然，我们从之前的实验中知道，正确的做法是降低学习率。但是，如果我们是第一次遇到这个问题，那么输出中就没有太多可以指导我们做什么了。我们可能不仅担心学习率，还担心我们神经网络的其他各个方面。我们可能想知道我们是否已经以使网络难以学习的方式初始化权重和偏差？或者我们没有足够的训练数据来获得有意义的学习？也许我们还没有跑足够的时间？或者，具有这种架构的神经网络不可能学会识别手写数字？可能学习率太低? 或者，也许，学习率太高了？当你第一次遇到问题时，你并不总是确定。</p>
<p>从中吸取的教训是，调试神经网络并非易事，而且就像普通编程一样，它是一门艺术。你需要学习调试的艺术才能从神经网络中获得好的结果。更一般地说，我们需要开发启发式方法来选择好的超参数和好的架构。我们将在本书中详细讨论所有这些，包括我如何选择上面的超参数。</p>
<p>之前，我跳过了如何加载 MNIST 数据的细节。这很简单。为了完整起见，这里是代码。用于存储 MNIST 数据的数据结构在文档字符串中进行了描述 - 它是简单的东西、元组和 <code>Numpy ndarray</code>对象列表（如果您不熟悉<code>ndarray</code> ，请将它们视为向量）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">mnist_loader</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A library to load the MNIST image data.  For details of the data</span></span><br><span class="line"><span class="string">structures that are returned, see the doc strings for ``load_data``</span></span><br><span class="line"><span class="string">and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the</span></span><br><span class="line"><span class="string">function usually called by our neural network code.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the MNIST data as a tuple containing the training data,</span></span><br><span class="line"><span class="string">    the validation data, and the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``training_data`` is returned as a tuple with two entries.</span></span><br><span class="line"><span class="string">    The first entry contains the actual training images.  This is a</span></span><br><span class="line"><span class="string">    numpy ndarray with 50,000 entries.  Each entry is, in turn, a</span></span><br><span class="line"><span class="string">    numpy ndarray with 784 values, representing the 28 * 28 = 784</span></span><br><span class="line"><span class="string">    pixels in a single MNIST image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The second entry in the ``training_data`` tuple is a numpy ndarray</span></span><br><span class="line"><span class="string">    containing 50,000 entries.  Those entries are just the digit</span></span><br><span class="line"><span class="string">    values (0...9) for the corresponding images contained in the first</span></span><br><span class="line"><span class="string">    entry of the tuple.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``validation_data`` and ``test_data`` are similar, except</span></span><br><span class="line"><span class="string">    each contains only 10,000 images.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is a nice data format, but for use in neural networks it&#x27;s</span></span><br><span class="line"><span class="string">    helpful to modify the format of the ``training_data`` a little.</span></span><br><span class="line"><span class="string">    That&#x27;s done in the wrapper function ``load_data_wrapper()``, see</span></span><br><span class="line"><span class="string">    below.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    f = gzip.<span class="built_in">open</span>(<span class="string">&#x27;../data/mnist.pkl.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    training_data, validation_data, test_data = cPickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_wrapper</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a tuple containing ``(training_data, validation_data,</span></span><br><span class="line"><span class="string">    test_data)``. Based on ``load_data``, but the format is more</span></span><br><span class="line"><span class="string">    convenient for use in our implementation of neural networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In particular, ``training_data`` is a list containing 50,000</span></span><br><span class="line"><span class="string">    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray</span></span><br><span class="line"><span class="string">    containing the input image.  ``y`` is a 10-dimensional</span></span><br><span class="line"><span class="string">    numpy.ndarray representing the unit vector corresponding to the</span></span><br><span class="line"><span class="string">    correct digit for ``x``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ``validation_data`` and ``test_data`` are lists containing 10,000</span></span><br><span class="line"><span class="string">    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional</span></span><br><span class="line"><span class="string">    numpy.ndarry containing the input image, and ``y`` is the</span></span><br><span class="line"><span class="string">    corresponding classification, i.e., the digit values (integers)</span></span><br><span class="line"><span class="string">    corresponding to ``x``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Obviously, this means we&#x27;re using slightly different formats for</span></span><br><span class="line"><span class="string">    the training data and the validation / test data.  These formats</span></span><br><span class="line"><span class="string">    turn out to be the most convenient for use in our neural network</span></span><br><span class="line"><span class="string">    code.&quot;&quot;&quot;</span></span><br><span class="line">    tr_d, va_d, te_d = load_data()</span><br><span class="line">    training_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> tr_d[<span class="number">0</span>]]</span><br><span class="line">    training_results = [vectorized_result(y) <span class="keyword">for</span> y <span class="keyword">in</span> tr_d[<span class="number">1</span>]]</span><br><span class="line">    training_data = <span class="built_in">zip</span>(training_inputs, training_results)</span><br><span class="line">    validation_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> va_d[<span class="number">0</span>]]</span><br><span class="line">    validation_data = <span class="built_in">zip</span>(validation_inputs, va_d[<span class="number">1</span>])</span><br><span class="line">    test_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> te_d[<span class="number">0</span>]]</span><br><span class="line">    test_data = <span class="built_in">zip</span>(test_inputs, te_d[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vectorized_result</span>(<span class="params">j</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a 10-dimensional unit vector with a 1.0 in the jth</span></span><br><span class="line"><span class="string">    position and zeroes elsewhere.  This is used to convert a digit</span></span><br><span class="line"><span class="string">    (0...9) into a corresponding desired output from the neural</span></span><br><span class="line"><span class="string">    network.&quot;&quot;&quot;</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br></pre></td></tr></table></figure>

<p>我上面说过，我们的程序得到了很好的结果。那是什么意思？好比什么？进行一些简单的（非神经网络）基线测试进行比较，以了解表现良好意味着什么，这是有益的。当然，最简单的基线是随机猜测数字。大约百分之十的时间是正确的。我们做得比这好得多！</p>
<p>一个不那么琐碎的基线呢？让我们尝试一个非常简单的想法：我们将看看图像有多暗。例如，一张图片<code>2</code>通常会比图像<code>1</code>的图像暗很多，只是因为更多的像素被涂黑，如下例所示：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_2_and_1.png" class="" title="数字2和数字1比较明暗度">

<p>这建议使用训练数据来计算每个数字的平均暗度，0,1,2,…,9. 当呈现新图像时，我们计算图像的暗度，然后猜测它是哪个数字具有最接近的平均暗度。这是一个简单的过程，并且很容易编写代码，所以我不会明确写出代码 - 如果您有兴趣，它在 GitHub 存储库中。但这比随机猜测有了很大的进步，得到2,225的10,000测试图像正确，即22.25 百分比准确度。</p>
<p>不难找到其他实现准确性的想法 20到50百分比范围。如果你再努力一点，你可以站起来50百分。但要获得更高的准确度，使用已建立的机器学习算法会有所帮助。让我们尝试使用最知名的算法之一，支持向量机 或SVM。如果您不熟悉 SVM，不用担心，我们不需要了解 SVM 如何工作的细节。相反，我们将使用一个名为 scikit-learn的 Python 库，它为称为 LIBSVM的 SVM 的基于 C 的快速库提供了一个简单的 Python 接口。</p>
<p>如果我们使用默认设置运行 scikit-learn 的 SVM 分类器，那么它会在 10,000 个测试图像中得到 9,435 个正确。（代码可 在此处获得。）与我们根据图像的暗度对图像进行分类的幼稚方法相比，这是一个很大的改进。事实上，这意味着 SVM 的性能大致与我们的神经网络一样好，只是差了一点。在后面的章节中，我们将介绍新技术，使我们能够改进我们的神经网络，使其性能比 SVM 好得多。</p>
<p>然而，这并不是故事的结局。10,000 个结果中的 9,435 个是 scikit-learn 对 SVM 的默认设置。SVM 有许多可调参数，并且可以搜索提高这种开箱即用性能的参数。我不会明确地进行此搜索，但 如果您想了解更多信息，请参阅Andreas Mueller的这篇博文。Mueller 表明，通过一些优化 SVM 参数的工作，可以将性能提高到 98.5% 以上的准确度。换句话说，一个经过良好调整的 SVM 只会在 70 中的一个数字上出错。这非常好！神经网络可以做得更好吗？</p>
<p>事实上，他们可以。目前，设计良好的神经网络优于其他所有解决 MNIST 的技术，包括 SVM。当前（2013 年）记录正确分类了 10,000 张图像中的 9,979 张。这是由Li Wan、Matthew Zeiler、Sixin Zhang、Yann LeCun和 Rob Fergus完成的。我们将在本书后面看到他们使用的大部分技术。在那个级别上，性能接近于人类，并且可以说更好，因为很多 MNIST 图像即使是人类也很难自信地识别，例如：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/mnist_really_bad_images.png" class="" title="mnist really bad images">

<p>我相信你会同意这些很难分类！有了 MNIST 数据集中的此类图像，神经网络可以准确地对 10,000 张测试图像中的 21 张以外的所有图像进行分类，这是非常了不起的。通常，在编程时，我们认为解决像识别 MNIST 数字这样的复杂问题需要复杂的算法。但即使是刚刚提到的 Wan等人的论文中的神经网络也涉及非常简单的算法，是我们在本章中看到的算法的变体。所有的复杂性都是从训练数据中自动学习的。从某种意义上说，我们的结果和更复杂论文中的结果的寓意在于，对于某些问题：</p>
<pre><code>复杂的算法≤简单的学习算法+良好的训练数据。
</code></pre>
<h2 id="走向深度学习"><a href="#走向深度学习" class="headerlink" title="走向深度学习"></a>走向深度学习</h2><p>虽然我们的神经网络提供了令人印象深刻的性能，但这种性能有点神秘。网络中的权重和偏差是自动发现的。这意味着我们无法立即解释网络是如何工作的。我们能否找到某种方法来理解我们的网络对手写数字进行分类的原理？而且，鉴于这些原则，我们能做得更好吗？</p>
<p>为了更明确地提出这些问题，假设几十年后神经网络导致人工智能 (AI)。我们会理解这种智能网络是如何工作的吗？也许网络对我们来说是不透明的，带有我们不理解的权重和偏差，因为它们是自动学习的。在人工智能研究的早期，人们希望构建人工智能的努力也能帮助我们理解智能背后的原理，也许还有人脑的功能。但也许结果是我们最终既不了解大脑，也不了解人工智能是如何工作的！</p>
<p>为了解决这些问题，让我们回想一下我在本章开头给出的人工神经元的解释，作为衡量证据的一种手段。假设我们要确定图像是否显示人脸：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/10.png" class="" title="Einstein crop">

<p>我们可以像攻击手写识别一样解决这个问题——通过使用图像中的像素作为神经网络的输入，网络的输出是单个神经元，指示“是的，它是一张脸”或“不，它是不是脸”。</p>
<p>假设我们这样做，但我们没有使用学习算法。相反，我们将尝试手动设计一个网络，选择适当的权重和偏差。我们该怎么做呢？暂时完全忘记神经网络，我们可以使用的启发式方法是将问题分解为子问题：图像左上角是否有眼睛？右上角有眼睛吗？中间有鼻子吗？它的底部中间有一个嘴吗？上面有头发吗？等等。</p>
<p>如果其中几个问题的答案是“是”，甚至只是“可能是”，那么我们会得出结论，该图像很可能是一张脸。相反，如果大多数问题的答案都是“否”，那么图像可能不是一张脸。</p>
<p>当然，这只是一种粗略的启发式，它存在许多不足之处。也许这个人是秃头，所以他们没有头发。可能我们只能看到脸部的一部分，或者脸部是有角度的，所以一些五官被遮挡了。尽管如此，启发式表明，如果我们可以使用神经网络解决子问题，那么也许我们可以通过组合子问题的网络来构建用于面部检测的神经网络。这是一个可能的架构，矩形表示子网络。请注意，这并不是解决人脸检测问题的现实方法。相反，它是为了帮助我们建立关于网络如何运作的直觉。这是架构：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/11.png" class="" title="network">

<p>子网络可以被分解也是合理的。假设我们正在考虑这个问题：“左上角有一只眼睛吗？” 这可以分解为诸如“有眉毛吗？”之类的问题；“有睫毛吗？”；“有虹膜吗？”；等等。当然，这些问题也应该包括位置信息——“眉毛在左上角，在虹膜上方吗？”之类的——但让我们保持简单。网络回答问题“左上角有眼睛吗？” 现在可以分解：</p>
<img src="/2023/03/30/Using-neural-nets-to-recognize-handwritten-digits/12.png" class="" title="sub network">

<p>这些问题也可以通过多个层次进一步分解。最终，我们将使用子网络来回答问题，这些问题非常简单，可以在单个像素级别轻松回答。例如，这些问题可能是关于图像中特定点是否存在非常简单的形状。这些问题可以通过连接到图像中原始像素的单个神经元来回答。</p>
<p>最终结果是一个网络，它将一个非常复杂的问题——这张图像是否显示一张脸——分解成非常简单的问题，可以在单个像素级别回答。它通过一系列的许多层来做到这一点，早期的层回答有关输入图像的非常简单和具体的问题，而后面的层则建立了一个更加复杂和抽象概念的层次结构。具有这种多层结构（两个或更多隐藏层）的网络称为深度神经网络。</p>
<p>当然，我还没有说如何将这种递归分解为子网络。手动设计网络中的权重和偏差当然是不切实际的。相反，我们希望使用学习算法，以便网络可以从训练数据中自动学习权重和偏差，从而学习概念的层次结构。1980 年代和 1990 年代的研究人员尝试使用随机梯度下降和反向传播来训练深度网络。不幸的是，除了一些特殊的架构之外，他们并没有太多的运气。网络会学习，但速度很慢，在实践中通常太慢而无法使用。</p>
<p>自 2006 年以来，已经开发了一套技术，可以在深度神经网络中进行学习。这些深度学习技术基于随机梯度下降和反向传播，但也引入了新思想。这些技术可以训练更深（和更大）的网络——人们现在经常训练具有 5 到 10 个隐藏层的网络。而且，事实证明，这些在许多问题上的性能都比浅层神经网络（即只有一个隐藏层的网络）要好得多。当然，原因是深度网络能够建立复杂的概念层次结构。这有点像传统编程语言使用模块化设计和抽象思想来创建复杂计算机程序的方式。将深度网络与浅层网络进行比较有点像将能够进行函数调用的编程语言与无法进行此类调用的精简语言进行比较。抽象在神经网络中采用与传统编程不同的形式，但它同样重要。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/03/RSA-Algorithm/" rel="prev" title="RSA Algorithm">
                  <i class="fa fa-chevron-left"></i> RSA Algorithm
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/12/Markdown-syntax/" rel="next" title="Markdown syntax">
                  Markdown syntax <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yurl</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"yurl2018/comments-yurl-blog","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
